{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cdKrjwFalhJJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in c:\\users\\anand\\.conda\\envs\\work\\lib\\site-packages (0.28.1)\n",
            "Requirement already satisfied: requests>=2.20 in c:\\users\\anand\\.conda\\envs\\work\\lib\\site-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\anand\\.conda\\envs\\work\\lib\\site-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\anand\\.conda\\envs\\work\\lib\\site-packages (from openai) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\anand\\.conda\\envs\\work\\lib\\site-packages (from requests>=2.20->openai) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\anand\\.conda\\envs\\work\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\anand\\.conda\\envs\\work\\lib\\site-packages (from requests>=2.20->openai) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anand\\.conda\\envs\\work\\lib\\site-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\anand\\.conda\\envs\\work\\lib\\site-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\anand\\.conda\\envs\\work\\lib\\site-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\anand\\.conda\\envs\\work\\lib\\site-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\anand\\.conda\\envs\\work\\lib\\site-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\anand\\.conda\\envs\\work\\lib\\site-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\anand\\.conda\\envs\\work\\lib\\site-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\anand\\.conda\\envs\\work\\lib\\site-packages (from tqdm->openai) (0.4.6)\n",
            "Collecting python-docx\n",
            "  Obtaining dependency information for python-docx from https://files.pythonhosted.org/packages/ea/82/ddb60b44c6e39a74bd406fab7d7c102ce7dfca2dff9515dfd6edc7d25f1e/python_docx-1.0.1-py3-none-any.whl.metadata\n",
            "  Downloading python_docx-1.0.1-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\anand\\.conda\\envs\\work\\lib\\site-packages (from python-docx) (4.9.3)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\anand\\.conda\\envs\\work\\lib\\site-packages (from python-docx) (4.7.1)\n",
            "Downloading python_docx-1.0.1-py3-none-any.whl (237 kB)\n",
            "   ---------------------------------------- 0.0/237.4 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/237.4 kB ? eta -:--:--\n",
            "   - -------------------------------------- 10.2/237.4 kB ? eta -:--:--\n",
            "   - -------------------------------------- 10.2/237.4 kB ? eta -:--:--\n",
            "   ----- --------------------------------- 30.7/237.4 kB 187.9 kB/s eta 0:00:02\n",
            "   ----- --------------------------------- 30.7/237.4 kB 187.9 kB/s eta 0:00:02\n",
            "   ------ -------------------------------- 41.0/237.4 kB 140.3 kB/s eta 0:00:02\n",
            "   ------ -------------------------------- 41.0/237.4 kB 140.3 kB/s eta 0:00:02\n",
            "   ---------- ---------------------------- 61.4/237.4 kB 182.2 kB/s eta 0:00:01\n",
            "   --------------- ----------------------- 92.2/237.4 kB 238.1 kB/s eta 0:00:01\n",
            "   ------------------ ------------------- 112.6/237.4 kB 273.1 kB/s eta 0:00:01\n",
            "   ------------------ ------------------- 112.6/237.4 kB 273.1 kB/s eta 0:00:01\n",
            "   ---------------------- --------------- 143.4/237.4 kB 283.8 kB/s eta 0:00:01\n",
            "   ------------------------ ------------- 153.6/237.4 kB 286.7 kB/s eta 0:00:01\n",
            "   ------------------------------- ------ 194.6/237.4 kB 328.0 kB/s eta 0:00:01\n",
            "   -------------------------------------- 237.4/237.4 kB 373.1 kB/s eta 0:00:00\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-1.0.1\n",
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "     ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/5.6 MB 217.9 kB/s eta 0:00:26\n",
            "     ---------------------------------------- 0.0/5.6 MB 217.9 kB/s eta 0:00:26\n",
            "     ---------------------------------------- 0.0/5.6 MB 217.9 kB/s eta 0:00:26\n",
            "     ---------------------------------------- 0.0/5.6 MB 140.3 kB/s eta 0:00:40\n",
            "     ---------------------------------------- 0.1/5.6 MB 192.5 kB/s eta 0:00:29\n",
            "      --------------------------------------- 0.1/5.6 MB 196.9 kB/s eta 0:00:29\n",
            "      --------------------------------------- 0.1/5.6 MB 238.1 kB/s eta 0:00:24\n",
            "      --------------------------------------- 0.1/5.6 MB 273.1 kB/s eta 0:00:21\n",
            "     - -------------------------------------- 0.1/5.6 MB 293.9 kB/s eta 0:00:19\n",
            "     - -------------------------------------- 0.2/5.6 MB 296.2 kB/s eta 0:00:19\n",
            "     - -------------------------------------- 0.2/5.6 MB 355.7 kB/s eta 0:00:16\n",
            "     - -------------------------------------- 0.3/5.6 MB 413.7 kB/s eta 0:00:13\n",
            "     -- ------------------------------------- 0.3/5.6 MB 442.4 kB/s eta 0:00:13\n",
            "     -- ------------------------------------- 0.3/5.6 MB 457.1 kB/s eta 0:00:12\n",
            "     -- ------------------------------------- 0.4/5.6 MB 530.1 kB/s eta 0:00:10\n",
            "     --- ------------------------------------ 0.5/5.6 MB 574.8 kB/s eta 0:00:09\n",
            "     --- ------------------------------------ 0.5/5.6 MB 617.9 kB/s eta 0:00:09\n",
            "     ---- ----------------------------------- 0.6/5.6 MB 715.2 kB/s eta 0:00:07\n",
            "     ---- ----------------------------------- 0.7/5.6 MB 735.5 kB/s eta 0:00:07\n",
            "     ----- ---------------------------------- 0.8/5.6 MB 827.4 kB/s eta 0:00:06\n",
            "     ------ --------------------------------- 0.9/5.6 MB 926.2 kB/s eta 0:00:06\n",
            "     ------- -------------------------------- 1.0/5.6 MB 983.3 kB/s eta 0:00:05\n",
            "     -------- ------------------------------- 1.2/5.6 MB 1.1 MB/s eta 0:00:05\n",
            "     --------- ------------------------------ 1.3/5.6 MB 1.2 MB/s eta 0:00:04\n",
            "     ----------- ---------------------------- 1.6/5.6 MB 1.4 MB/s eta 0:00:03\n",
            "     ------------ --------------------------- 1.8/5.6 MB 1.5 MB/s eta 0:00:03\n",
            "     -------------- ------------------------- 2.0/5.6 MB 1.6 MB/s eta 0:00:03\n",
            "     -------------- ------------------------- 2.1/5.6 MB 1.6 MB/s eta 0:00:03\n",
            "     ------------------ --------------------- 2.6/5.6 MB 2.0 MB/s eta 0:00:02\n",
            "     --------------------- ------------------ 3.0/5.6 MB 2.2 MB/s eta 0:00:02\n",
            "     ------------------------- -------------- 3.5/5.6 MB 2.5 MB/s eta 0:00:01\n",
            "     ---------------------------- ----------- 4.1/5.6 MB 2.8 MB/s eta 0:00:01\n",
            "     ------------------------------ --------- 4.2/5.6 MB 2.8 MB/s eta 0:00:01\n",
            "     ----------------------------------- ---- 5.0/5.6 MB 3.3 MB/s eta 0:00:01\n",
            "     ---------------------------------------  5.6/5.6 MB 3.6 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 5.6/5.6 MB 3.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\anand\\.conda\\envs\\work\\lib\\site-packages (from pdfminer.six) (3.3.0)\n",
            "Collecting cryptography>=36.0.0 (from pdfminer.six)\n",
            "  Obtaining dependency information for cryptography>=36.0.0 from https://files.pythonhosted.org/packages/d7/78/29d8332bebfe3c2d49a63fb23e1c9fc73a13507b5206b98479fda04c993b/cryptography-41.0.4-cp37-abi3-win_amd64.whl.metadata\n",
            "  Downloading cryptography-41.0.4-cp37-abi3-win_amd64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in c:\\users\\anand\\.conda\\envs\\work\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six) (1.15.1)\n",
            "Requirement already satisfied: pycparser in c:\\users\\anand\\.conda\\envs\\work\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
            "Downloading cryptography-41.0.4-cp37-abi3-win_amd64.whl (2.7 MB)\n",
            "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
            "   ----------------------------- ---------- 1.9/2.7 MB 60.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.7/2.7 MB 56.4 MB/s eta 0:00:00\n",
            "Installing collected packages: cryptography, pdfminer.six\n",
            "Successfully installed cryptography-41.0.4 pdfminer.six-20221105\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install python-docx\n",
        "!pip install pdfminer.six"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pandasNote: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/2d/5e/9213ea10ac473e2437dc2cb17323ddc0999997e2713d6a0b683b10773994/pandas-2.1.1-cp311-cp311-win_amd64.whl.metadata\n",
            "  Using cached pandas-2.1.1-cp311-cp311-win_amd64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\anand\\.conda\\envs\\work\\lib\\site-packages (from pandas) (1.26.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\anand\\.conda\\envs\\work\\lib\\site-packages (from pandas) (2.8.2)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Obtaining dependency information for pytz>=2020.1 from https://files.pythonhosted.org/packages/32/4d/aaf7eff5deb402fd9a24a1449a8119f00d74ae9c2efa79f8ef9994261fc2/pytz-2023.3.post1-py2.py3-none-any.whl.metadata\n",
            "  Using cached pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.1 (from pandas)\n",
            "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\anand\\.conda\\envs\\work\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Using cached pandas-2.1.1-cp311-cp311-win_amd64.whl (10.6 MB)\n",
            "Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
            "Installing collected packages: pytz, tzdata, pandas\n",
            "Successfully installed pandas-2.1.1 pytz-2023.3.post1 tzdata-2023.3\n"
          ]
        }
      ],
      "source": [
        "pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YWoNzjDOlh2F"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import docx\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from pdfminer.high_level import extract_text\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cJyI-2yKlkTI"
      },
      "outputs": [],
      "source": [
        "api_key = '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'\n",
        "openai.api_key = api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Converting the given resume files into text files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CcVYlG-Plktt"
      },
      "outputs": [],
      "source": [
        "def convert_pdf_to_text2(file_path):\n",
        "    try:\n",
        "        text = extract_text(file_path)\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from PDF {file_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def convert_docx_to_text(file_path):\n",
        "    try:\n",
        "        doc = docx.Document(file_path)\n",
        "        text = ''\n",
        "        for paragraph in doc.paragraphs:\n",
        "            text += paragraph.text + '\\\\n'\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from DOCX {file_path}: {e}\")\n",
        "        return \"\"\n",
        "def convert_txt_to_text(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            content = file.read()\n",
        "        return content\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {e}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "osUjc7Efll9n"
      },
      "outputs": [],
      "source": [
        "def get_choice_text_from_prompt(messages):\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo-16k\",\n",
        "            messages=messages,\n",
        "            temperature=0,\n",
        "            max_tokens=4000\n",
        "        )\n",
        "        choice_text = response.choices[0][\"message\"][\"content\"]\n",
        "        return choice_text\n",
        "    except Exception as e:\n",
        "        print(\"Error in get_choice_text_from_prompt:\", str(e))\n",
        "        return \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iG3cA2iSlsqf"
      },
      "outputs": [],
      "source": [
        "def truncate_text_by_words(text, max_words=4000):\n",
        "    \"\"\"\n",
        "    Truncates the text to a specified number of words.\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    if len(words) <= max_words:\n",
        "        return text\n",
        "    return \" \".join(words[:max_words])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "scKiexFhls_T"
      },
      "outputs": [],
      "source": [
        "def parse_resume(resumetext):\n",
        "    try:\n",
        "        # Ensure resume text does not exceed 4000 tokens\n",
        "        # resumetext = truncate_text_by_words(resumetext, 4000)\n",
        "        resumetext = truncate_text_by_words(resumetext, 4000)\n",
        "        system = \"\"\"\n",
        "        You are an excellent NLP engineer, skilled talent recruiter and data scientist and your task is to analyse and parse candidate resumes into meaningful structured JSON format.\n",
        "\n",
        "        You will be provided with candidate resume text.\n",
        "\n",
        "        The system instruction is:\n",
        "\n",
        "        Step-1:\n",
        "        Analyse and parse the following information from the candidate's resume, do not just extract the data, rephrase it meaningfully;\n",
        "        return the meaningful parsed data in a sturctured JSON format with key and corresponding value format as follows-\n",
        "        'name': string,\n",
        "        'gmail': string,\n",
        "        'phone number' : string,\n",
        "        'social media links': list of string,\n",
        "        'skillset and expertise': list of string,\n",
        "        'certifications': list of string,\n",
        "        'Explanation of projects': list of string under 200 tokens,\n",
        "        'Explanation of position of responsibilities': list of string under 200 tokens,\n",
        "        'years of experience': string,\n",
        "        'Previous work experience description': list of string under 200 tokens,\n",
        "        'educational qualification': list of string,\n",
        "        'extracurriculars': list of string,\n",
        "        'awards and achievements': list of string,\n",
        "        'previous job title': list of string\n",
        "        If value of a key is missing in the resume then value should be null.\n",
        "        If not a resume then all the key's value should be null.\n",
        "\n",
        "        Step-2:\n",
        "        Only return the parsed JSON format resume, nothing else.\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"\n",
        "        Only return the structured parsed json format of the resume of candidate.\n",
        "        Information about the candidate's resume is given inside text delimited by triple backticks.\n",
        "\n",
        "\n",
        "        Candidate's Resume :```{resumetext}```\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        messages =  [\n",
        "        {'role':'system', 'content':system},\n",
        "        {'role':'user', 'content': prompt}]\n",
        "\n",
        "        parsed_resume = get_choice_text_from_prompt(messages)\n",
        "        return parsed_resume\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing resume: {e}\")\n",
        "        return \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now dealing with my resume file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "cv_txt = convert_pdf_to_text2('cv_final.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "json_txt = parse_resume(cv_txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'{\\n  \"name\": \"ANANDU N R\",\\n  \"gmail\": null,\\n  \"phone number\": null,\\n  \"social media links\": null,\\n  \"skillset and expertise\": [\\n    \"C\",\\n    \"C++\",\\n    \"Python\",\\n    \"Matlab\",\\n    \"SQL\"\\n  ],\\n  \"certifications\": [\\n    \"Google Advanced Data Analytics Professional Certificate | Google (Coursera)\"\\n  ],\\n  \"Explanation of projects\": [\\n    \"Selection of Best Playing 11 of the T20 Cricket World Cup 2022 | Self Project\",\\n    \"Classification of CIFAR 10 dataset using ANN and CNN\"\\n  ],\\n  \"Explanation of position of responsibilities\": [\\n    \"Secretary Social and Cultural | Vidyasagar Hall of Residence\"\\n  ],\\n  \"years of experience\": null,\\n  \"Previous work experience description\": [\\n    \"Summer Research Internship | FT | Stanford University\",\\n    \"Freelance Math Expert | Photomath\"\\n  ],\\n  \"educational qualification\": [\\n    \"M.TECH Dual Degree 5Y\",\\n    \"Higher Secondary Examination\",\\n    \"SSLC Examination\"\\n  ],\\n  \"extracurriculars\": [\\n    \"[Inter IIT] Integral team member of Gold Winning Athletics team of IIT Kharagpur in the 55th Inter IIT Sports Meet 2022 and also secured 2 gold medals in my respective events\",\\n    \"[Inter Hall] Secured a total of 4 medals, including 1 Gold and 3 Silver for VS Hall of Residence in Inter Hall Athletics GC 2023 which led to an overall Silver Position\",\\n    \"[Inter Hall] Secured 4 Gold medals for VS Hall of Residence in Inter Hall Athletics Championship 2022, leading to overall Gold Position\",\\n    \"Selected for NCA Instrumental Music program, IIT Kharagpur for the academic session 2020-2021 & 2021-2022\"\\n  ],\\n  \"awards and achievements\": [\\n    \"Awarded Athletics Excellence Alumni Award for the Academic Year 2022-23 from the Gymkhana for exceptional performance in the Inter IIT Sports Meet 2022\",\\n    \"Awarded 3rd position at the Open IIT Eastern Vocals held at IIT Kharagpur in 2020-2021\",\\n    \"Secured rank 45 out of over 75,000 candidates in the 2020 Kerala State Engineering Entrance Examination\"\\n  ],\\n  \"previous job title\": null\\n}'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "json_txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Resume is converted into the structured json file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating doc file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Required libraries\n",
        "import json\n",
        "from docx import Document\n",
        "from docx.shared import Pt\n",
        "from docx.shared import RGBColor\n",
        "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
        "from docx.enum.style import WD_STYLE_TYPE\n",
        "\n",
        "# Function for creating a doc from json file\n",
        "def create_doc_from_json(json_txt, filename=\"resume.docx\"):\n",
        "    json_data = json.loads(json_txt)\n",
        "    doc = Document()\n",
        "\n",
        "    # Defining some styles ( Main heading style, sub headings, content style)\n",
        "    # Main heading (for 'Resume')\n",
        "    main_heading_style = doc.styles.add_style('MainHeadingStyle', WD_STYLE_TYPE.PARAGRAPH)\n",
        "    main_heading_style.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
        "    main_heading_style.font.size = Pt(16)\n",
        "    main_heading_style.font.name = 'Times New Roman'\n",
        "    main_heading_style.font.color.rgb = RGBColor(0, 0, 255) # In blue color\n",
        "\n",
        "    # Headings(Keys of the json file)\n",
        "    heading_style = doc.styles.add_style('HeadingStyle', WD_STYLE_TYPE.PARAGRAPH)\n",
        "    heading_style.font.size = Pt(14)\n",
        "    heading_style.font.name = 'Times New Roman'\n",
        "    heading_style.font.color.rgb = RGBColor(0, 0, 255)  # In blue color\n",
        "    heading_style.font.bold = True\n",
        "\n",
        "    # Content style, which is not required in bullet points\n",
        "    Content_style = doc.styles.add_style('ContentStyle', WD_STYLE_TYPE.PARAGRAPH)\n",
        "    Content_style.font.size = Pt(12)\n",
        "    Content_style.font.name = 'Times New Roman'\n",
        "\n",
        "    # Adding the centered heading \"Resume\" as the main heading\n",
        "    centered_heading = doc.add_paragraph(\"Resume\", style='MainHeadingStyle')\n",
        "    centered_heading.runs[0].bold = True\n",
        "    centered_heading.runs[0].underline = True\n",
        "\n",
        "    for key, value in json_data.items():\n",
        "        # Giving the 'HeadingStyle' to all the keys in the json file\n",
        "        heading = doc.add_paragraph(key.capitalize(), style='HeadingStyle')\n",
        "        heading.runs[0].underline = True\n",
        "\n",
        "        # Since some values are null, we have to take care of that\n",
        "        \n",
        "        if value is not None and isinstance(value, list):\n",
        "                for item in value:\n",
        "                    # Using the default 'ListBullet' style for content for adding a bullet point at the beginning of each values\n",
        "                    content = doc.add_paragraph(style='ListBullet')\n",
        "                    content.add_run(item)\n",
        "        elif value is not None:\n",
        "            # Splitting the value into paragraphs based on '\\n'\n",
        "            paragraphs = value.split('\\n')\n",
        "\n",
        "            for paragraph in paragraphs:\n",
        "                if paragraph.strip():\n",
        "                    # Using 'ContentStyle' style for contents which are not in lists\n",
        "                    content = doc.add_paragraph(style='ContentStyle')\n",
        "                    content.add_run(paragraph.strip())\n",
        "        else:\n",
        "             # Writing as 'Null', if the values are null and also using the ContentStyle for writing\n",
        "             content = doc.add_paragraph(style='ContentStyle')\n",
        "             content.add_run('Null')\n",
        "\n",
        "    doc.save(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\anand\\.conda\\envs\\work\\Lib\\site-packages\\docx\\styles\\styles.py:130: UserWarning: style lookup by style_id is deprecated. Use style name as key instead.\n",
            "  return self._get_style_id_from_style(self[style_name], style_type)\n"
          ]
        }
      ],
      "source": [
        "create_doc_from_json(json_txt,'resume1.docx')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
