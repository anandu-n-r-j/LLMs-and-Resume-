{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "api_key = \"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\"\n",
    "\n",
    "openai.api_key = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_parser(jd):\n",
    "    system='''You are an excellent skilled talent recruiter, your task is to provide a concise structured summary of the given job description in a  JSON format.\n",
    "        You will be provided with job description.\n",
    "        The system instruction is:\n",
    "        Step-1:\n",
    "        Analyse and parse the following information from the job description, do not just extract the data, rephrase it meaningfully;\n",
    "        return the meaningful parsed data in a sturctured JSON format with key and corresponding value format as follows-\n",
    "        'Role':string\n",
    "        'Relevant Experiences required': string,\n",
    "        'Experience Duration required': string,\n",
    "        'Skillset and Tools required' : string,\n",
    "        'Project description': string,\n",
    "        'Responsibilities required': string,\n",
    "        'Certifications required': string\n",
    "        'Education required': string\n",
    "        If value of a key is missing in the job description then value should be null.\n",
    "        If not a job description then all the key's value should be null.\n",
    "        Step-2:\n",
    "        Only return the parsed JSON format resume, nothing else.'''\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "            Only return the structured parsed json format of the job description.\n",
    "            Information about the job description is given inside text delimited by triple backticks.\n",
    "\n",
    "            job description :```{jd}```\n",
    "\n",
    "            \"\"\"\n",
    "    # Record the start time\n",
    "    start_time = time.time()\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-16k\", \n",
    "        #model=\"gpt-4\"\n",
    "        messages=[{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": prompt}]\n",
    "    )[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "    # Parse the JSON response\n",
    "    try:\n",
    "        output_json = json.loads(completion)\n",
    "    except json.JSONDecodeError as e:\n",
    "        st.error(f\"Error parsing JSON: {e}\")\n",
    "        return None\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate the elapsed time\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Time Taken for Parsing : {elapsed_time} seconds\")\n",
    "    return output_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Jd_text=f'''Job description\n",
    "Overview of the Role: \n",
    "\n",
    "As a Machine Learning Engineer, you will be responsible for the Design and Development of Machine Learning Systems as well as refining and updating the existing systems. You will be responsible for bringing the best software development practices to the data science team and helping them speed up their work, Test machine learning libraries to their extremes, often adding new functionalities. Enabling production deployment of code, testability, and accuracy metric tracking. You may have to look constantly for performance improvement and decide which ML technologies will be used in a production environment.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Key Responsibilities:\n",
    "\n",
    "Design and develop machine learning algorithms and deep learning applications and systems for Kloud-9. Solve complex problems with multilayered data sets, and optimize existing machine learning libraries and frameworks. Collaborate with data scientists, administrators, data analysts, data engineers, and data architects on production systems and applications\n",
    " \n",
    "The candidate should be highly skilled in statistics and programming, with the ability to confidently assess, analyse, and organize large amounts of data. Identify differences in data distribution that could potentially affect model performance in real-world applications.\n",
    " \n",
    "Ensure algorithms generate accurate user recommendations. Stay up to date with developments in the machine learning industry. Implement ML platform capabilities to streamline all phases of data-centric innovation, including data access and exploration, model development, productionisation, testing, and monitoring of machine learning pipelines.\n",
    " \n",
    "Design and own end-to-end ML platforms that enable ML Applied Scientists with model and feature pipeline development, deployment, monitoring, and maintenance. Builds & maintains machine learning and big data production pipelines to support advanced analytics, data science, and AI/ML solutions.\n",
    " \n",
    "Identifies valuable internal and external data. Collaborates closely with data and ML scientists to define data for the design, development, and deployment of new solutions that support strategic business priorities. Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and intelligence to support business needs\n",
    " \n",
    "Attributes & Competencies:\n",
    "\n",
    "Proficiency with Python and machine learning libraries such as scikit-learn, matplotlib, seaborn and pandas\n",
    "Knowledge of Big Data frameworks like Hadoop, Spark, Pig, Hive, Flume, etc\n",
    "Experience in working with ML frameworks like TensorFlow, Keras, OpenCV\n",
    "Expertise in visualizing and manipulating big datasets\n",
    "Familiarity with Linux\n",
    "Ability to select hardware to run an ML model with the required latency\n",
    "Robust data modelling and data architecture skills.\n",
    "Advanced degree in Computer Science/Math/Statistics or a related discipline.\n",
    "Advanced Math and Statistics skills (linear algebra, calculus, Bayesian statistics, mean, median, variance, etc.)\n",
    "Exploring and visualizing data to gain an understanding of it, then identifying differences in\n",
    "data distribution that could affect performance when deploying the model in the real world\n",
    "Verifying data quality, and/or ensuring it via data cleaning\n",
    "Supervising the data acquisition process if more data is needed\n",
    "Finding available datasets online that could be used for training\n",
    "\n",
    "\n",
    "Skills:\n",
    "\n",
    "Python, Flask, Pyspark (Spark-Core, Spark-SQL, Spark-ML)\n",
    "Scikit-learn, matplotlib, seaborn, pandas, OpenCV, Keras, Tensorflowm, Scala,\n",
    "Jupyter Notebook, Machine Learning, Deep Learning\n",
    "GCP components - Cloud Functions, Cloud Storage, DataProc, Google Kubernetes Engine (GKE), Vertex AI, Compute Engine\n",
    "Kube Flow & Kubernetes\n",
    "\n",
    "\n",
    "Experience: 3 Years+\n",
    "Education: Bachelor's or Master's in Computer Science, Data Science, Machine Learning, or a related field'\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken for Parsing : 18.82324504852295 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Role': 'Machine Learning Engineer',\n",
       " 'Relevant Experiences required': 'Proficiency with Python and machine learning libraries such as scikit-learn, matplotlib, seaborn, and pandas\\nExperience in working with ML frameworks like TensorFlow, Keras, OpenCV\\nFamiliarity with Linux\\nRobust data modeling and data architecture skills',\n",
       " 'Experience Duration required': '3 Years+',\n",
       " 'Skillset and Tools required': 'Python, Flask, Pyspark (Spark-Core, Spark-SQL, Spark-ML)\\nScikit-learn, matplotlib, seaborn, pandas, OpenCV, Keras, TensorFlowm, Scala,\\nJupyter Notebook, Machine Learning, Deep Learning\\nGCP components - Cloud Functions, Cloud Storage, DataProc, Google Kubernetes Engine (GKE), Vertex AI, Compute Engine\\nKube Flow & Kubernetes',\n",
       " 'Project description': 'Design and develop machine learning algorithms and deep learning applications and systems for Kloud-9. Solve complex problems with multilayered data sets, and optimize existing machine learning libraries and frameworks. Collaborate with data scientists, administrators, data analysts, data engineers, and data architects on production systems and applications',\n",
       " 'Responsibilities required': 'Collaborate with data scientists, administrators, data analysts, data engineers, and data architects on production systems and applications\\nEnsure algorithms generate accurate user recommendations.\\nImplement ML platform capabilities to streamline all phases of data-centric innovation\\nDesign and own end-to-end ML platforms that enable ML Applied Scientists\\nIdentify valuable internal and external data\\nDevelop large-scale data structures and pipelines',\n",
       " 'Certifications required': None,\n",
       " 'Education required': \"Bachelor's or Master's in Computer Science, Data Science, Machine Learning, or a related field\"}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_parser(Jd_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "system1='''You are an excellent skilled talent recruiter, your task is to provide a concise structured summary of the given job description in a  JSON format.\n",
    "        You will be provided with job description.\n",
    "        The system instruction is:\n",
    "        Step-1:\n",
    "        Analyse and parse the following information from the job description, do not just extract the data, rephrase it meaningfully;\n",
    "        return the meaningful parsed data in a sturctured JSON format with key and corresponding value format as follows-\n",
    "        'Role':string\n",
    "        'Relevant Experiences required': string,\n",
    "        'Experience Duration required': string,\n",
    "        'Skillset and Tools required' : string,\n",
    "        If value of a key is missing in the job description then value should be null.\n",
    "        If not a job description then all the key's value should be null.\n",
    "        Step-2:\n",
    "        Only return the parsed JSON format resume, nothing else.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "system2='''You are an excellent skilled talent recruiter, your task is to provide a concise structured summary of the given job description in a  JSON format.\n",
    "        You will be provided with job description.\n",
    "        The system instruction is:\n",
    "        Step-1:\n",
    "        Analyse and parse the following information from the job description, do not just extract the data, rephrase it meaningfully;\n",
    "        return the meaningful parsed data in a sturctured JSON format with key and corresponding value format as follows-\n",
    "        'Project description': string,\n",
    "        'Responsibilities required': string,\n",
    "        'Certifications required': string,\n",
    "        'Education required': string,\n",
    "        If value of a key is missing in the job description then value should be null.\n",
    "        If not a job description then all the key's value should be null.\n",
    "        Step-2:\n",
    "        Only return the parsed JSON format resume, nothing else.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting httpx\n",
      "  Obtaining dependency information for httpx from https://files.pythonhosted.org/packages/33/0d/d9ce469af019741c8999711d36b270ff992ceb1a0293f73f9f34fdf131e9/httpx-0.25.0-py3-none-any.whl.metadata\n",
      "  Downloading httpx-0.25.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\anand\\.conda\\envs\\work\\lib\\site-packages (from httpx) (2023.7.22)\n",
      "Collecting httpcore<0.19.0,>=0.18.0 (from httpx)\n",
      "  Obtaining dependency information for httpcore<0.19.0,>=0.18.0 from https://files.pythonhosted.org/packages/ac/97/724afbb7925339f6214bf1fdb5714d1a462690466832bf8fb3fd497649f1/httpcore-0.18.0-py3-none-any.whl.metadata\n",
      "  Downloading httpcore-0.18.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\anand\\.conda\\envs\\work\\lib\\site-packages (from httpx) (3.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\anand\\.conda\\envs\\work\\lib\\site-packages (from httpx) (1.2.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\anand\\.conda\\envs\\work\\lib\\site-packages (from httpcore<0.19.0,>=0.18.0->httpx) (3.5.0)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore<0.19.0,>=0.18.0->httpx)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
      "     --------------------- ------------------ 30.7/58.3 kB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 58.3/58.3 kB 1.0 MB/s eta 0:00:00\n",
      "Downloading httpx-0.25.0-py3-none-any.whl (75 kB)\n",
      "   ---------------------------------------- 0.0/75.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 75.7/75.7 kB ? eta 0:00:00\n",
      "Downloading httpcore-0.18.0-py3-none-any.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/76.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 76.0/76.0 kB 4.1 MB/s eta 0:00:00\n",
      "Installing collected packages: h11, httpcore, httpx\n",
      "Successfully installed h11-0.14.0 httpcore-0.18.0 httpx-0.25.0\n"
     ]
    }
   ],
   "source": [
    "!pip install httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest_asyncio in c:\\users\\anand\\.conda\\envs\\work\\lib\\site-packages (1.5.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Role': 'Machine Learning Engineer', 'Relevant Experiences required': 'Design and development of machine learning algorithms and deep learning applications and systems, optimization of existing machine learning libraries and frameworks, collaboration with data scientists, administrators, data analysts, data engineers, and data architects on production systems and applications', 'Experience Duration required': '3 Years+', 'Skillset and Tools required': \"Proficiency with Python and machine learning libraries such as scikit-learn, matplotlib, seaborn, and pandas, knowledge of Big Data frameworks like Hadoop, Spark, Pig, Hive, Flume, etc, experience in working with ML frameworks like TensorFlow, Keras, OpenCV, expertise in visualizing and manipulating big datasets, familiarity with Linux, ability to select hardware to run an ML model with the required latency, robust data modeling and data architecture skills, advanced degree in Computer Science/Math/Statistics or a related discipline, advanced Math and Statistics skills, exploring and visualizing data, verifying data quality, supervising the data acquisition process, finding available datasets online, Python, Flask, Pyspark, Scikit-learn, matplotlib, seaborn, pandas, OpenCV, Keras, Tensorflow, Scala, Jupyter Notebook, Machine Learning, Deep Learning, GCP components - Cloud Functions, Cloud Storage, DataProc, Google Kubernetes Engine (GKE), Vertex AI, Compute Engine, Kube Flow & Kubernetes, Bachelor's or Master's in Computer Science, Data Science, Machine Learning, or a related field\", 'Project description': 'As a Machine Learning Engineer, you will be responsible for the Design and Development of Machine Learning Systems as well as refining and updating the existing systems. You will be responsible for bringing the best software development practices to the data science team and helping them speed up their work, Test machine learning libraries to their extremes, often adding new functionalities. Enabling production deployment of code, testability, and accuracy metric tracking. You may have to look constantly for performance improvement and decide which ML technologies will be used in a production environment.', 'Responsibilities required': 'Design and develop machine learning algorithms and deep learning applications and systems for Kloud-9. Solve complex problems with multilayered data sets, and optimize existing machine learning libraries and frameworks. Collaborate with data scientists, administrators, data analysts, data engineers, and data architects on production systems and applications. Ensure algorithms generate accurate user recommendations. Stay up to date with developments in the machine learning industry. Implement ML platform capabilities to streamline all phases of data-centric innovation, including data access and exploration, model development, productionisation, testing, and monitoring of machine learning pipelines. Design and own end-to-end ML platforms that enable ML Applied Scientists with model and feature pipeline development, deployment, monitoring, and maintenance. Builds & maintains machine learning and big data production pipelines to support advanced analytics, data science, and AI/ML solutions. Identifies valuable internal and external data. Collaborates closely with data and ML scientists to define data for the design, development, and deployment of new solutions that support strategic business priorities. Develops large scale data structures and pipelines to organize, collect and standardize data that helps generate insights and intelligence to support business needs.', 'Certifications required': None, 'Education required': \"Bachelor's or Master's in Computer Science, Data Science, Machine Learning, or a related field\"}\n",
      "Execution time: 20.30 seconds\n"
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import openai\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Get OpenAI API key from environment variable\n",
    "API_KEY = \"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\"\n",
    "\n",
    "jd = Jd_text\n",
    "systems = [system1, system2]\n",
    "\n",
    "\n",
    "async def async_openai_request(session, jd, system):\n",
    "    url = \"https://api.openai.com/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    prompt = f\"\"\"\n",
    "            Only return the structured parsed json format of the job description.\n",
    "            Information about the job description is given inside text delimited by triple backticks.\n",
    "\n",
    "            job description :```{jd}```\n",
    "\n",
    "            \"\"\"\n",
    "    data = {\n",
    "        \"model\": \"gpt-3.5-turbo-16k\",\n",
    "        \"messages\": [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": 0\n",
    "    }\n",
    "    async with session.post(url, json=data, headers=headers) as response:\n",
    "        return await response.json()\n",
    "\n",
    "async def fetch_responses():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        results = await asyncio.gather(*(async_openai_request(session, jd, system) for system in systems))\n",
    "    return results\n",
    "\n",
    "def process_responses(responses):\n",
    "    output_list = [json.loads(resp['choices'][0]['message']['content']) for resp in responses]   \n",
    "    # Merging dictionaries\n",
    "    combined_dict = {k: v for response in output_list for k, v in response.items()}\n",
    "    return combined_dict\n",
    "\n",
    "async def main():\n",
    "    responses = await fetch_responses()\n",
    "    return process_responses(responses)\n",
    "\n",
    "# Benchmarking\n",
    "start_time = time.time()\n",
    "print(await main())\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Execution time: {elapsed_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
